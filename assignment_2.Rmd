---
title: "Assignment_2"
author: "Chuyuan LI"
date: "10/11/2018"
output: html_document
---

```{r setup}
library(ggplot2)
library(dplyr)
source("functions.R")
```

```{r}
`%>%` <- magrittr::`%>%`
```

## Exercise 1
First do some permutation tests for the two subgroups:
```{r cache=TRUE}
# extract subsets from iris
iris_subset_1 <- iris[c(89:94, 108:112),]
iris_subset_2 <- iris[c(88:114),]

# do permutation tests
ptest_subset1 <- permutation_twogroups(iris_subset_1, "Sepal.Width", "Species", "versicolor",
"virginica", difference_in_medians)
ptest_subset2 <- permutation_twogroups(iris_subset_2, "Sepal.Width", "Species", "versicolor",
"virginica", difference_in_medians)
```

Now plot the histograms
```{r}
# extract the values observed and permuted statistics
observed_value_subset1 <- ptest_subset1["observed"]
ptest_stats_subset1 <- tibble::as_tibble(ptest_subset1["permuted"])
observed_value_subset2 <- ptest_subset2["observed"]
ptest_stats_subset2 <- tibble::as_tibble(ptest_subset2["permuted"])

# plot with the same x scale
lower_lim <- min(ptest_stats_subset1, ptest_stats_subset2)
upper_lim <- max(ptest_stats_subset1, ptest_stats_subset2)
ggplot(ptest_stats_subset1, aes(x=permuted)) + geom_histogram(color="white", fill="blue") + labs(title = "Histogram of observed value and permuted values subset1", x = "permuted values", y = "count") + geom_vline(aes(xintercept=unlist(observed_value_subset1), color = "observed_value"), show.legend = T, size = 1) + xlim(lower_lim, upper_lim)

ggplot(ptest_stats_subset2, aes(x=permuted)) + geom_histogram(color="white", fill="blue") + labs(title = "Histogram of observed value and permuted values subset2", x = "permuted values", y = "count") + geom_vline(aes(xintercept=unlist(observed_value_subset2), color = "observed_value"), show.legend = T, size = 1) + xlim(lower_lim, upper_lim)

```


**Observation from the 2 histograms**:
```
- Statistics for subset1 scattered towards lower and higher tails; while those for subset2 gather up to the center, more symetrique than subset1.
- There are many blank intervals in between the values for subset1, for exemple (-0.25,0); while for subset 2 they are more equally distributed with less blanks.
- Subset1 takes 11 points ans subset2 takes 27. When adding points, there are more different combinaisons, so the difference of medians varies more, that's the reason there are less blank bins in subset2 and the graphe is more sysmetrique

- Observed test statistics for subset 1 is -0.3, for subset 2 is -0.25
- Subset2 has an observed value bigger than subset1, meaning that the average Sepal.Width of subset2 is more alike with that of subset1
- The observed value is a one-chance value, meaning it is not the most representative among all the possible permutation statistics (too extreme at the lower side or upper side). 

```

Calculate 2-sided **p-value** for the 2 tests:
```{r}
p_value_left_subset1 <- p_value_mc_left(ptest_stats_subset1, observed_value_subset1)
p_value_right_subset1 <- p_value_mc_right(ptest_stats_subset1, observed_value_subset1)
cat(p_value_left_subset1, p_value_right_subset1, sep = " ", fill = TRUE)

p_value_left_subset2 <- p_value_mc_left(ptest_stats_subset2, observed_value_subset2)
p_value_right_subset2 <- p_value_mc_right(ptest_stats_subset2, observed_value_subset2)
cat(p_value_left_subset2, p_value_right_subset2, sep = " ", fill = TRUE)
```

**Comments on two-sided p-value**:
```
- Left-sided test: the probability of permuted value is smaller than observed value
- Right-sided test: the probability of permuted value is larger than observed value
- The results of both 2 tests show that left sided p-value is much smaller that right-sided p-value (0.778 > 0.275, 0.944 > 0.128). Compared to observed values of subset1 -0.3, more than 77% permutated values are large than -0.3; as for subset2, more than 94% of permutated values are larger than observed value -0.25. 
- the right-sided p-value of subset2 is bigger than that of subset 2 (0.944 > 0.778), meaning the observed value of subset2 is less likely to be found in the permutation test (< 6% chance that it occurs at the observed position). 
```

## Exercise 2
```{r}
#install.packages("devtools")
devtools::install_github("ewan/stats_course", subdir="data/stress_shift")
```

### Task A
```{r}
df_unamb <- stressshift::stress_shift_unamb
stress_shift_3dict <- dplyr::filter(df_unamb, Dict %in% c("W1802", "J1917", "C1687"))
print(nrow(stress_shift_3dict))
```

### Task B
```{r}
stress_shift_3dict_using_pipe <- df_unamb %>% dplyr::filter(Dict %in% c("W1802", "J1917", "C1687"))
identical(stress_shift_3dict, stress_shift_3dict_using_pipe)
```

### Task C
```{r}
stress_shift_3dict_nouns <- dplyr::filter(stress_shift_3dict, Category=="Noun")
stress_shift_3dict_verbs <- dplyr::filter(stress_shift_3dict, Category=="Verb")

stress_shift_3dict_using_bind <- dplyr::bind_rows(stress_shift_3dict_nouns, stress_shift_3dict_verbs)
stress_shift_3dict_using_bind_reversed <- dplyr::bind_rows(stress_shift_3dict_verbs, stress_shift_3dict_nouns)

#check which of the two binded tables is identical with stress_shift_3dict
identical(stress_shift_3dict_using_bind, stress_shift_3dict) 
identical(stress_shift_3dict_using_bind_reversed, stress_shift_3dict)
```

The first binded table `stress_shift_3dict_using_bind` is identical with the `stress_shift_3dict` and the reversed one is not:

- When create the table `stress_shift_3dict`, the original dataframe sorted the category albaphetcally, which means nouns appear in front of the verbs. 
- When bind the separated dataframs `stress_shift_3dict_nouns` and `stress_shift_3dict_verbs` together, fonction `bind_rows` takes the first argument infront of the second one to create a new dataframe. 
- fonction `identical()` checks the exactly equality between two arguemnts. When comparing `stress_shift_3dict_using_bind` and `stress_shift_3dict`, they have the same ordering, so identical; while `stress_shift_3dict_using_bind_reversed` is not because the odering of Category is not the same.
- It wouldn't matter which one I'm working with since the content of the two tables is the same, I could use fonction `arrange(Category)` if needed to change the inner order.


### Task D
```{r}
stress_shift_nouns_renamed <- df_unamb %>% 
  dplyr::filter(Category=="Noun") %>%
  dplyr::select(Word, Dict, Syllable) %>%
  dplyr::rename(Syllable_Noun=Syllable)
  
stress_shift_verbs_renamed <- df_unamb %>% 
  dplyr::filter(Category=="Verb") %>%
  dplyr::select(Word, Dict, Syllable) %>%
  dplyr::rename(Syllable_Verb=Syllable)

stress_shift_wide <- dplyr::inner_join(stress_shift_nouns_renamed, stress_shift_verbs_renamed)

# check the rows of stress_shift_wide and the 2 renamed tables
n_v_rows <- nrow(stress_shift_nouns_renamed) + nrow(stress_shift_verbs_renamed)
wide_rows <- nrow(stress_shift_wide)
cat(nrow(stress_shift_nouns_renamed),nrow(stress_shift_verbs_renamed), n_v_rows, wide_rows)
# head(stress_shift_wide)
```

By definition, `stress_shift_wide(x, y)` is a condensed table that takes all the rows in table x where there are matching values in y and all colomns in x and y. In this case, the two renamed tables each contains 3 colomns: Word, Dict, Syllable_Category with the first 2 colomns the same name, the only difference is the third colomn. When doing an inner_join, the result is `joined by c("Word", "Dict")` which means it only take the **rows that share the same value of "Word" and "Dict"** in both of the two tables and make it only one row, that's why it has fewer rows than any one of the two original tables.


### Task E
```{r}
# create a summarised table which sums up the counts of category and syllable combinasion
df_unamb_summarised <- df_unamb %>% group_by(Category, Syllable) %>% summarise(Value=n())
print(df_unamb_summarised)
ggplot2::ggplot(df_unamb_summarised, ggplot2::aes_string(fill="Syllable", x="Category", y="Value")) +
  geom_bar(position="dodge", stat="identity") + ggplot2::ggtitle("Double bar plot for stress_shift_unamb")

# do the same plot for stress_shift_permit
df_permit <- stressshift::stress_shift_permit
df_permit_summarised <- df_permit %>% group_by(Category, Syllable) %>% summarise(Value=n())
ggplot2::ggplot(df_permit_summarised, ggplot2::aes_string(fill="Syllable", x="Category", y="Value")) +
  geom_bar(position="dodge", stat="identity") + ggplot2::ggtitle("Double bar plot for stress_shift_permit")

```

### Task F
```{r}
stress_shift_group_by_word <- stress_shift_wide %>% group_by(Word) %>% summarise(Total = n())
#print(stress_shift_byword2)

stress_shift_noun_syll_1 <- stress_shift_wide %>% filter(Syllable_Noun == "Syllable 1") %>% group_by(Word) %>% summarise(Count_noun = n())
#print(stress_shift_noun_syll_1)

stress_shift_verb_syll_1 <- stress_shift_wide %>% filter(Syllable_Verb == "Syllable 1") %>% group_by(Word) %>% summarise(Count_verb = n())
#print(stress_shift_verb_syll_1)

left_joined_groups <- left_join(stress_shift_group_by_word, stress_shift_verb_syll_1, by="Word") %>% left_join(., stress_shift_noun_syll_1, by="Word")
# print(left_joined_groups)

stress_shift_byword <- left_joined_groups %>% mutate(Noun_Percent_Syll_1 = Count_noun / Total, Verb_Percent_Syll_1 = Count_verb / Total) %>% select(Word, Noun_Percent_Syll_1, Verb_Percent_Syll_1)
print(stress_shift_byword)
print(nrow(stress_shift_byword) == 149)
```

### Task G
```{r}
ggplot2::ggplot(stress_shift_byword, aes(x=Noun_Percent_Syll_1, y=Verb_Percent_Syll_1)) + geom_point(shape=1) + labs(title = "Scatterplot of syllable 1 in category noun v.s verb in each word")
```

### Task H
```{r}
n_groupby_word <- stressshift::stress_shift_unamb %>% filter(Category == "Noun") %>% group_by(Word) %>% summarise(Noun_total = n())
v_groupby_word <- stressshift::stress_shift_unamb %>% filter(Category == "Verb") %>% group_by(Word) %>% summarise(Verb_total = n())
stress_shift_full_wide <- full_join(n_groupby_word, v_groupby_word)
# print(stress_shift_full_wide)

n_syll1_groupby_word <- stressshift::stress_shift_unamb %>% filter(Category == "Noun", Syllable == "Syllable 1") %>% group_by(Word) %>% summarise(Count_n_syll1 = n())
v_syll1_groupby_word <- stressshift::stress_shift_unamb %>% filter(Category == "Verb", Syllable == "Syllable 1") %>% group_by(Word) %>% summarise(Count_v_syll1 = n())

stress_shift_byword_all <- left_join(stress_shift_full_wide, n_syll1_groupby_word) %>% left_join(., v_syll1_groupby_word) %>% mutate(Noun_Percent_Syll_1 = Count_n_syll1 / Noun_total, Verb_Percent_Syll_1 = Count_v_syll1 / Verb_total) %>% select(Word, Noun_Percent_Syll_1, Verb_Percent_Syll_1)
print(stress_shift_byword_all)
```


## Exercise 3

```{r}
set.seed(12)
# Create 2 normal distributions
sample_a1 <- rnorm(50, 3, 2)
sample_b1 <- rnorm(50, 4, 2)
set.seed(NULL)
# create a dataframe with 2 colomns
df1 <- tibble::data_frame(value=c(sample_a1, sample_b1), group=c(rep("A", 50), rep("B",50)))
# t-test based on df
t.test(dplyr::filter(df1, group == "A")$value, dplyr::filter(df1, group == "B")$value)
```
**Explanation 1**

The result tells that the t-value equals to -4.196 with a degree of freedom at 97.99. 
We can say that at 95% confidence level, there IS significant difference (p-value = 5.974e-05) of the two means. Hence we should reject the null hypothesis that the two means are equal because the p-value is much smaller than 0.05. The maximum difference of the mean can be as low as -2.13 and as high as -0.76. It also shows that the estimated sample mean for x is 2.71 and that for sample_b is 4.16 (which is reasonable because we set them at 3 and 4 respectively).


Now re-do the t-test with 5 samples in each of the group:
```{r}
set.seed(12)
sample_a2 <- rnorm(5, 3, 2)
sample_b2 <- rnorm(5, 4, 2)
set.seed(NULL)

df2 <- tibble::data_frame(value=c(sample_a2, sample_b2), group=c(rep("A", 5), rep("B",5)))
t.test(dplyr::filter(df2, group == "A")$value, dplyr::filter(df2, group == "B")$value)
```
**Explanation 2**

Here at 95% confidence level, there is NO significant difference (p-value = 0.1577) of the two means. So we should accept the null hypothesis that the two means are equal because the p-value is larger than 0.05. The maximum difference of the mean can be as low as -5.52 and as high as 1.21.


**Why different**

For experiment 1 we reject the null hypothesis, while for the experiment 2 we accepte it because the seconde p-value is larger than 0.05. With other parameters stay the same, the reason for which is that we reduce the number of observations from 50 to 5.  Run the permutation test:


```{r cache=TRUE}
statistics1 <- permutation_t_test(df1, 'value', 'group', 'A', 'B')
statistics2 <- permutation_t_test(df2, 'value', 'group', 'A', 'B')
```


Plot the results with reference to the original statistics:
```{r}
hist(statistics1, freq = F, breaks = 80, xlim = range(-4.2,4.2))
abline(v=t.test(sample_a1, sample_b1)$statistic, col='red')
hist(statistics2, freq = F, breaks = 80, xlim = range(-4.2,4.2))
abline(v=t.test(sample_a2, sample_b2)$statistic, col='red')
```

**Observation**

- within the same x-limit, statistics 2 has more blank ranges than that of statistics 1, which means there are more missing combinations due to the small number of observations
- the original statistic line for permutation test 1 lies **outside** the histogram of range (-4, 4) which means that the chance are so small that under null hypothesis we could find a value also extreme as -4.196, that's why we reject the null hypothesis
- while the original statistic permuation test 2 lines inside the histogram with a relative high probability (p-value>0.05) to observe this value, that's why we accepte the null hypothesis


## Exercise 4

```{r cache=TRUE}
sample_size <- c(50, 5)
standard_deviation <- c(2, 6)
diff_in_mean <- c(1, 2)
power_df <- NULL # a dataframe to show the statistical power for each combination

for (size in sample_size){
  for (sd in standard_deviation){
    for (diff in diff_in_mean){
      
      N_TEST <- 9999
      n_successes <- 0
      for (i in 1: N_TEST){
        sample_a <- rnorm(size, 3, sd)
        sample_b <- rnorm(size, 3+diff, sd)
        df <- tibble::data_frame(value=c(sample_a, sample_b), group=c(rep("A", size), rep("B",size)))
        pval <- t.test(dplyr::filter(df, group == "A")$value, dplyr::filter(df, group == "B")$value)$p.value
        if (pval < 0.05){
          n_successes <- n_successes + 1
        }
      }
      power <- n_successes / N_TEST
      power_df_current <- tibble::data_frame(power=power, size=size, sd=sd, diff=diff)
      power_df <- dplyr::bind_rows(power_df, power_df_current)
    }
  }
}

```

Now plot the results with tidyr and ggplot2:
```{r}
power_df %>%
  tidyr::gather(-power, key = "var", value = "value") %>% 
  ggplot2::ggplot(aes(x = value, y = power)) +
    geom_point() +
    facet_wrap(~ var, scales = "free") +
    theme_bw()
```

**Explanation**

- The use of library tidyr aims at treating each variable by itself, and observe their influence on the statistical power
- The use of facet_wrap and theme_by from ggplot2 aims at combine the separate graphs into a complete one.

- Horizontally, there are 3 points shown in the **"same line"** in the 3 sub-graphs respectively, which represent 1 possible combination (for exemple, on the top line with the power = 0.999, the corresponding combination is: diff=2, sd=2, size=50). In total there are 8 'lines', so 8 combinations; from up to bottom, we can see the different combinations yield different statistical powers
- In each of the sub-graph, compare a **paired points** from left to right, say in the sub-graph diff, we can observe the 2 highest points: one with diff=1, power=0.698 on the left, and one with diff=2, power=0.999 on the right. These paired two points have the *same sd and size*, only differ by diff. Obviously, diff=2 yields a better power value than diff=1 (we can check for the rest 3 paired points in this sub-graph). The same reasoning goes for sub-graph sd and size. In short:

  - diff=2 gives higher power than diff=1
  - standard deviation=2 gives higher power than sd=6
  - size=50 gives higher power than size=5
  
- In conclusion, a bigger size of samples, with a smaller standard deviation and a bigger
difference in the means yield a higher statistical power in the resampling test.





